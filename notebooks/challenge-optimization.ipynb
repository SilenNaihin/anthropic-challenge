{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Performance Engineering Challenge: A Deep Dive\n",
    "\n",
    "You have been given a simple kernel that runs in 147,734 cycles. The goal is to make it run in ~1,500 cycles.\n",
    "\n",
    "That is a **100x speedup**.\n",
    "\n",
    "How is that even possible? The answer lies in understanding how modern processors actually work, and then systematically exploiting every available resource. By the end of this notebook, you will understand not just *what* optimizations to apply, but *why* each one works and *how much* speedup it can deliver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Does the Kernel Actually Do?\n",
    "\n",
    "Before we can optimize anything, we need to deeply understand the problem. Let us trace through what the kernel computes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import the problem definitions\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from problem import (\n",
    "    Tree, Input, HASH_STAGES, myhash,\n",
    "    reference_kernel, build_mem_image,\n",
    "    SLOT_LIMITS, VLEN, SCRATCH_SIZE\n",
    ")\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel traverses a binary tree. But not just once - it does this for **256 independent traversals** running in parallel, each repeating for **16 rounds**.\n",
    "\n",
    "Let us visualize the tree structure first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_binary_tree(height):\n",
    "    \"\"\"Show how a binary tree is stored as a flat array.\"\"\"\n",
    "    # Binary tree stored as array:\n",
    "    # index 0 = root\n",
    "    # left child of i = 2*i + 1\n",
    "    # right child of i = 2*i + 2\n",
    "    \n",
    "    total_nodes = 2**(height+1) - 1\n",
    "    print(f\"Tree height: {height}\")\n",
    "    print(f\"Total nodes: {total_nodes}\")\n",
    "    print()\n",
    "    \n",
    "    # Show tree structure for small height\n",
    "    if height <= 3:\n",
    "        for level in range(height + 1):\n",
    "            start = 2**level - 1\n",
    "            count = 2**level\n",
    "            indices = list(range(start, start + count))\n",
    "            spacing = \" \" * (2**(height - level + 1))\n",
    "            nodes = spacing.join(f\"[{i}]\" for i in indices)\n",
    "            print(f\"Level {level}: {' ' * (2**(height - level))} {nodes}\")\n",
    "    else:\n",
    "        print(f\"(Tree too large to display, {height} levels)\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Navigation:\")\n",
    "    print(\"  From node i:\")\n",
    "    print(\"    Left child  = 2*i + 1\")\n",
    "    print(\"    Right child = 2*i + 2\")\n",
    "\n",
    "visualize_binary_tree(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Traversal Algorithm\n",
    "\n",
    "Each traversal works like this:\n",
    "\n",
    "1. Start at some node with some value\n",
    "2. XOR your value with the node's stored value\n",
    "3. Hash the result (this is the expensive part)\n",
    "4. If the hash is even, go left; if odd, go right\n",
    "5. If you fall off the tree, wrap back to the root\n",
    "6. Repeat\n",
    "\n",
    "Let us trace through a single step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_one_step(tree_values, current_idx, current_val):\n",
    "    \"\"\"Trace through one step of the traversal, showing all operations.\"\"\"\n",
    "    n_nodes = len(tree_values)\n",
    "    \n",
    "    print(f\"Current position: node {current_idx}\")\n",
    "    print(f\"Current value:    0x{current_val:08x}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 1: Load node value\n",
    "    node_val = tree_values[current_idx]\n",
    "    print(f\"1. Load node value: tree[{current_idx}] = 0x{node_val:08x}\")\n",
    "    \n",
    "    # Step 2: XOR\n",
    "    xored = current_val ^ node_val\n",
    "    print(f\"2. XOR with value:  0x{current_val:08x} ^ 0x{node_val:08x} = 0x{xored:08x}\")\n",
    "    \n",
    "    # Step 3: Hash (expensive!)\n",
    "    hashed = myhash(xored)\n",
    "    print(f\"3. Hash result:     myhash(0x{xored:08x}) = 0x{hashed:08x}\")\n",
    "    \n",
    "    # Step 4: Compute next index\n",
    "    is_even = (hashed % 2 == 0)\n",
    "    direction = \"LEFT\" if is_even else \"RIGHT\"\n",
    "    offset = 1 if is_even else 2\n",
    "    next_idx = 2 * current_idx + offset\n",
    "    print(f\"4. Direction:       {'even' if is_even else 'odd'} -> go {direction}\")\n",
    "    print(f\"   Next index:      2 * {current_idx} + {offset} = {next_idx}\")\n",
    "    \n",
    "    # Step 5: Boundary check\n",
    "    if next_idx >= n_nodes:\n",
    "        print(f\"5. Boundary check:  {next_idx} >= {n_nodes} -> WRAP to root\")\n",
    "        next_idx = 0\n",
    "    else:\n",
    "        print(f\"5. Boundary check:  {next_idx} < {n_nodes} -> OK\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Result: node {next_idx}, value 0x{hashed:08x}\")\n",
    "    return next_idx, hashed\n",
    "\n",
    "# Create a small tree and trace one step\n",
    "random.seed(42)\n",
    "small_tree = Tree.generate(3)  # 15 nodes\n",
    "trace_one_step(small_tree.values, 0, 0x12345678)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hash Function: Where the Cycles Go\n",
    "\n",
    "The hash function is not arbitrary - it has 6 stages, each with the same structure. Understanding this structure is crucial for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_hash_stages():\n",
    "    \"\"\"Show what each hash stage computes.\"\"\"\n",
    "    print(\"The hash function has 6 stages:\")\n",
    "    print()\n",
    "    print(\"Each stage computes: a = (a op1 const1) op2 (a op3 shift_amount)\")\n",
    "    print()\n",
    "    \n",
    "    for i, (op1, val1, op2, op3, val3) in enumerate(HASH_STAGES):\n",
    "        print(f\"Stage {i}: a = (a {op1} 0x{val1:08x}) {op2} (a {op3} {val3})\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Operations per stage:\")\n",
    "    print(\"  - 1 arithmetic/logic op (+ or ^) with constant\")\n",
    "    print(\"  - 1 shift operation\")\n",
    "    print(\"  - 1 combining operation (+ or ^)\")\n",
    "    print(\"  - 1 32-bit truncation (implicit)\")\n",
    "    print()\n",
    "    print(f\"Total: 6 stages x 3 operations = 18 ALU operations per hash\")\n",
    "\n",
    "explain_hash_stages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Operations in the Inner Loop\n",
    "\n",
    "Now let us count exactly how many operations happen in each iteration of the inner loop. This tells us the theoretical minimum work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_inner_loop_operations():\n",
    "    \"\"\"Count all operations in one inner loop iteration.\"\"\"\n",
    "    ops = {\n",
    "        \"load\": 0,\n",
    "        \"store\": 0,\n",
    "        \"alu\": 0,\n",
    "        \"flow\": 0\n",
    "    }\n",
    "    \n",
    "    print(\"Operations per batch item per round:\")\n",
    "    print()\n",
    "    \n",
    "    # Load idx and val from input arrays\n",
    "    print(\"Loading inputs:\")\n",
    "    print(\"  load idx from memory       (1 load)\")\n",
    "    print(\"  load val from memory       (1 load)\")\n",
    "    ops[\"load\"] += 2\n",
    "    \n",
    "    # Load node value (dependent on idx)\n",
    "    print(\"  compute idx address        (1 alu: +)\")\n",
    "    print(\"  load node_val from tree    (1 load)\")\n",
    "    ops[\"alu\"] += 1\n",
    "    ops[\"load\"] += 1\n",
    "    \n",
    "    # XOR\n",
    "    print(\"Computing hash input:\")\n",
    "    print(\"  val ^ node_val             (1 alu: ^)\")\n",
    "    ops[\"alu\"] += 1\n",
    "    \n",
    "    # Hash - 6 stages, 3 ops each\n",
    "    print(\"Hash function:\")\n",
    "    print(\"  6 stages x 3 ops           (18 alu)\")\n",
    "    ops[\"alu\"] += 18\n",
    "    \n",
    "    # Compute next index\n",
    "    print(\"Computing next index:\")\n",
    "    print(\"  val % 2                    (1 alu: %)\")\n",
    "    print(\"  val % 2 == 0               (1 alu: ==)\")\n",
    "    print(\"  select 1 or 2              (1 flow: select)\")\n",
    "    print(\"  2 * idx                    (1 alu: *)\")\n",
    "    print(\"  idx + offset               (1 alu: +)\")\n",
    "    ops[\"alu\"] += 4\n",
    "    ops[\"flow\"] += 1\n",
    "    \n",
    "    # Boundary check\n",
    "    print(\"Boundary check:\")\n",
    "    print(\"  idx < n_nodes              (1 alu: <)\")\n",
    "    print(\"  select idx or 0            (1 flow: select)\")\n",
    "    ops[\"alu\"] += 1\n",
    "    ops[\"flow\"] += 1\n",
    "    \n",
    "    # Store results\n",
    "    print(\"Storing outputs:\")\n",
    "    print(\"  compute store addresses    (2 alu: +)\")\n",
    "    print(\"  store idx                  (1 store)\")\n",
    "    print(\"  store val                  (1 store)\")\n",
    "    ops[\"alu\"] += 2\n",
    "    ops[\"store\"] += 2\n",
    "    \n",
    "    print()\n",
    "    print(\"Summary:\")\n",
    "    for op, count in ops.items():\n",
    "        print(f\"  {op}: {count}\")\n",
    "    print(f\"  TOTAL: {sum(ops.values())} operations\")\n",
    "    \n",
    "    return ops\n",
    "\n",
    "inner_ops = count_inner_loop_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Full Scale: Why 147,734 Cycles?\n",
    "\n",
    "Now we can understand why the baseline is so slow. The test case processes:\n",
    "- 256 batch items\n",
    "- 16 rounds each\n",
    "- ~30 operations per (item, round) pair\n",
    "\n",
    "Let us calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_baseline_performance():\n",
    "    \"\"\"Understand why the baseline takes 147,734 cycles.\"\"\"\n",
    "    \n",
    "    BATCH_SIZE = 256\n",
    "    ROUNDS = 16\n",
    "    BASELINE_CYCLES = 147734\n",
    "    \n",
    "    # Operations per inner loop iteration\n",
    "    ops_per_iteration = 30  # approximately\n",
    "    \n",
    "    total_iterations = BATCH_SIZE * ROUNDS\n",
    "    total_operations = total_iterations * ops_per_iteration\n",
    "    \n",
    "    print(\"Test case parameters:\")\n",
    "    print(f\"  Batch size:  {BATCH_SIZE}\")\n",
    "    print(f\"  Rounds:      {ROUNDS}\")\n",
    "    print(f\"  Tree height: 10 (2047 nodes)\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Total inner loop iterations: {BATCH_SIZE} x {ROUNDS} = {total_iterations}\")\n",
    "    print(f\"Operations per iteration: ~{ops_per_iteration}\")\n",
    "    print(f\"Total operations: ~{total_operations:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Baseline cycles: {BASELINE_CYCLES:,}\")\n",
    "    cycles_per_iteration = BASELINE_CYCLES / total_iterations\n",
    "    print(f\"Cycles per iteration: {cycles_per_iteration:.1f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"The baseline uses one instruction per cycle.\")\n",
    "    print(f\"That is about {cycles_per_iteration:.0f} cycles for {ops_per_iteration} operations.\")\n",
    "    print(\"Extra cycles come from address calculations and setup.\")\n",
    "\n",
    "analyze_baseline_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine: What Resources Do We Have?\n",
    "\n",
    "Now comes the key insight. The baseline is slow because it uses only a tiny fraction of available resources. Let us see what the machine offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_machine_resources():\n",
    "    \"\"\"Display all available engine slots.\"\"\"\n",
    "    \n",
    "    print(\"Available resources per cycle:\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    \n",
    "    for engine, slots in SLOT_LIMITS.items():\n",
    "        if engine == \"debug\":\n",
    "            continue  # Not relevant for performance\n",
    "        bar = \"#\" * slots\n",
    "        print(f\"{engine:8s}: [{bar:<12}] {slots} slots\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Key facts:\")\n",
    "    print(f\"  - VLEN = {VLEN} (vector operations process {VLEN} elements)\")\n",
    "    print(f\"  - Scratch space: {SCRATCH_SIZE} words of fast memory\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate theoretical throughput\n",
    "    scalar_alu_per_cycle = SLOT_LIMITS[\"alu\"]\n",
    "    vector_alu_per_cycle = SLOT_LIMITS[\"valu\"] * VLEN\n",
    "    loads_per_cycle = SLOT_LIMITS[\"load\"]\n",
    "    stores_per_cycle = SLOT_LIMITS[\"store\"]\n",
    "    \n",
    "    print(\"Theoretical throughput per cycle:\")\n",
    "    print(f\"  - Scalar ALU operations: {scalar_alu_per_cycle}\")\n",
    "    print(f\"  - Vector ALU elements:   {vector_alu_per_cycle} ({SLOT_LIMITS['valu']} ops x {VLEN} elements)\")\n",
    "    print(f\"  - Memory loads:          {loads_per_cycle}\")\n",
    "    print(f\"  - Memory stores:         {stores_per_cycle}\")\n",
    "\n",
    "show_machine_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Baseline Uses Almost Nothing\n",
    "\n",
    "The baseline implementation (shown below) uses **one slot per cycle**. That means it wastes:\n",
    "- 11 out of 12 scalar ALU slots\n",
    "- All 6 vector ALU slots\n",
    "- 1 out of 2 load slots\n",
    "- 1 out of 2 store slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_baseline_utilization():\n",
    "    \"\"\"Visualize how poorly the baseline uses resources.\"\"\"\n",
    "    \n",
    "    print(\"Baseline resource utilization per cycle:\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    \n",
    "    # In the baseline, each instruction uses exactly 1 slot\n",
    "    utilization = {\n",
    "        \"alu\": 1,   # Uses 1 of 12\n",
    "        \"valu\": 0,  # Uses 0 of 6\n",
    "        \"load\": 1,  # Uses 1 of 2\n",
    "        \"store\": 1, # Uses 1 of 2\n",
    "        \"flow\": 1   # Uses 1 of 1\n",
    "    }\n",
    "    \n",
    "    for engine, slots in SLOT_LIMITS.items():\n",
    "        if engine == \"debug\":\n",
    "            continue\n",
    "        used = utilization.get(engine, 0)\n",
    "        pct = (used / slots) * 100\n",
    "        used_bar = \"#\" * used\n",
    "        unused_bar = \".\" * (slots - used)\n",
    "        print(f\"{engine:8s}: [{used_bar}{unused_bar:<12}] {used}/{slots} = {pct:5.1f}%\")\n",
    "    \n",
    "    print()\n",
    "    print(\"The baseline wastes most of the machine's capacity!\")\n",
    "    print(\"This is why 100x speedup is possible.\")\n",
    "\n",
    "show_baseline_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## The Critical Bottleneck: Memory, Not Compute\n\nBefore we dive into optimizations, we need to identify what will ultimately limit our performance. This is crucial because it tells us where to focus our efforts.\n\nLook at the slot counts again:\n- **12 ALU slots** + **6 vALU slots** (each processing 8 elements) = massive compute capacity\n- **2 load slots** + **2 store slots** = very limited memory bandwidth\n\nThe kernel does 8 scattered tree loads per vector group. With only 2 load slots, that is **4 cycles minimum** just for tree loads, no matter how fast our compute is.\n\nThis has a profound implication: **memory is the bottleneck, not compute**. The hash function takes 12 cycles due to dependencies, but we only need 4 cycles of loads. If we can overlap the loads with the hash computation, memory becomes the limiting factor.\n\nTarget milestone breakdown (keep these in mind):\n- **Scalar baseline**: 147,734 cycles (1 op per cycle)\n- **Vectorized (8x SIMD)**: ~7,700 cycles (8 items at once)\n- **+ VLIW packing**: ~3,500 cycles (multiple ops per cycle)\n- **+ Software pipelining**: ~1,500 cycles (overlap memory and compute)\n\nEach optimization builds on the previous. Let us see how.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Strategy 1: Vectorization (SIMD)\n",
    "\n",
    "The most important observation: **the 256 batch items are completely independent**. We can process 8 of them simultaneously using vector operations.\n",
    "\n",
    "This is not a new idea - you learned about SIMD in the parallelism notebook. The key insight here is that SIMD works perfectly for this problem because:\n",
    "\n",
    "1. Each batch item follows the same operations\n",
    "2. There are no dependencies between batch items\n",
    "3. The batch size (256) is a multiple of VLEN (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_vectorization_opportunity():\n",
    "    \"\"\"Show how vectorization transforms the computation.\"\"\"\n",
    "    \n",
    "    BATCH_SIZE = 256\n",
    "    \n",
    "    print(\"Scalar approach (baseline):\")\n",
    "    print(f\"  Process 1 batch item at a time\")\n",
    "    print(f\"  {BATCH_SIZE} iterations per round\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Vector approach (VLEN = {VLEN}):\")\n",
    "    print(f\"  Process {VLEN} batch items at a time\")\n",
    "    print(f\"  {BATCH_SIZE // VLEN} iterations per round\")\n",
    "    print()\n",
    "    \n",
    "    speedup = BATCH_SIZE / (BATCH_SIZE // VLEN)\n",
    "    print(f\"Potential speedup from vectorization: {speedup}x\")\n",
    "    print()\n",
    "    \n",
    "    print(\"What changes:\")\n",
    "    print(\"  Scalar:  val = myhash(val ^ node_val)   // 1 value\")\n",
    "    print(\"  Vector:  vals[0:8] = myhash(vals[0:8] ^ node_vals[0:8])  // 8 values\")\n",
    "\n",
    "analyze_vectorization_opportunity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Catch: Scattered Memory Access\n",
    "\n",
    "There is one complication. While the batch `values` array is contiguous (we can use `vload`/`vstore`), the tree nodes we need to load are **scattered** - each batch item might be at a different tree position.\n",
    "\n",
    "This means we cannot use `vload` for tree values. We need 8 separate scalar loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_scattered_access():\n",
    "    \"\"\"Show why tree access is scattered.\"\"\"\n",
    "    \n",
    "    print(\"Memory access patterns:\")\n",
    "    print()\n",
    "    print(\"Batch indices (contiguous - can use vload):\")\n",
    "    print(\"  inp_indices[0], inp_indices[1], ..., inp_indices[7]\")\n",
    "    print(\"  These are consecutive memory locations\")\n",
    "    print()\n",
    "    print(\"Batch values (contiguous - can use vload/vstore):\")\n",
    "    print(\"  inp_values[0], inp_values[1], ..., inp_values[7]\")\n",
    "    print(\"  These are consecutive memory locations\")\n",
    "    print()\n",
    "    print(\"Tree values (SCATTERED - must use scalar loads):\")\n",
    "    print(\"  tree[indices[0]], tree[indices[1]], ..., tree[indices[7]]\")\n",
    "    print(\"  Example: tree[0], tree[15], tree[7], tree[3], tree[1023], ...\")\n",
    "    print(\"  These are NON-consecutive memory locations\")\n",
    "    print()\n",
    "    print(\"Impact on slots:\")\n",
    "    print(f\"  We have only 2 load slots per cycle\")\n",
    "    print(f\"  Loading {VLEN} scattered tree values needs {VLEN} loads\")\n",
    "    print(f\"  That takes {VLEN // 2} cycles just for tree loads\")\n",
    "\n",
    "explain_scattered_access()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Cycle Count Estimate\n",
    "\n",
    "Let us estimate how many cycles a vectorized implementation would take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def estimate_vectorized_cycles():\n    \"\"\"Estimate cycles after vectorization.\"\"\"\n    \n    BATCH_SIZE = 256\n    ROUNDS = 16\n    BASELINE = 147734\n    \n    vector_groups = BATCH_SIZE // VLEN  # 32 groups of 8\n    \n    print(\"Per vector group (8 batch items):\")\n    print()\n    \n    # Count cycles for each operation\n    cycles = {}\n    \n    # Load batch indices - vload takes 1 slot, 1 cycle\n    cycles[\"load batch indices\"] = 1\n    print(f\"  Load 8 indices (vload):        1 cycle\")\n    \n    # Load batch values - vload takes 1 slot, 1 cycle  \n    cycles[\"load batch values\"] = 1\n    print(f\"  Load 8 values (vload):         1 cycle\")\n    \n    # Load tree values - 8 scattered loads, 2 per cycle\n    tree_load_cycles = (VLEN + 1) // 2  # ceiling division\n    cycles[\"load tree values\"] = tree_load_cycles\n    print(f\"  Load 8 tree values (scattered): {tree_load_cycles} cycles\")\n    \n    # XOR - 1 valu op\n    cycles[\"xor\"] = 1\n    print(f\"  XOR values with tree values:   1 cycle\")\n    \n    # Hash - 6 stages, each stage has a dependency chain\n    # Each stage: compute tmp1 and tmp2 in parallel (1 cycle), then combine (1 cycle)\n    # So each stage takes 2 cycles minimum\n    hash_cycles = 12  # 6 stages x 2 cycles per stage (dependency limited)\n    cycles[\"hash\"] = hash_cycles\n    print(f\"  Hash 6 stages (2 cycles each): {hash_cycles} cycles\")\n    \n    # Compute next index - several valu ops\n    cycles[\"next index\"] = 2\n    print(f\"  Compute next index:            ~2 cycles\")\n    \n    # Store - vstore for values, scattered for indices\n    cycles[\"store\"] = tree_load_cycles + 1\n    print(f\"  Store results:                 ~{cycles['store']} cycles\")\n    \n    total_per_group = sum(cycles.values())\n    print()\n    print(f\"  Total per vector group: ~{total_per_group} cycles\")\n    print()\n    \n    total = vector_groups * ROUNDS * total_per_group\n    print(f\"Total estimated cycles:\")\n    print(f\"  {vector_groups} groups x {ROUNDS} rounds x {total_per_group} cycles = {total:,}\")\n    print()\n    print(f\"Speedup over baseline: {BASELINE / total:.1f}x\")\n    print()\n    print(\"This is still far from the 1,500 cycle target!\")\n    print(\"But wait - we identified the bottleneck earlier: MEMORY.\")\n    print(\"The hash takes 12 cycles, but we only have 4 cycles of loads.\")\n    print(\"Can we overlap them?\")\n    \n    return total\n\nvectorized_estimate = estimate_vectorized_cycles()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Strategy 2: Instruction Packing (VLIW)\n",
    "\n",
    "Recall from the parallelism notebook: VLIW lets us issue multiple independent operations in the same cycle. The baseline issues ONE operation per cycle. We can do much better.\n",
    "\n",
    "The key is to find operations that:\n",
    "1. Do not depend on each other\n",
    "2. Use different engine slots (or fit within slot limits)\n",
    "3. Are needed at approximately the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_vliw_packing_example():\n",
    "    \"\"\"Demonstrate how to pack multiple operations per cycle.\"\"\"\n",
    "    \n",
    "    print(\"Baseline (1 op per cycle):\")\n",
    "    print(\"  Cycle 1: load indices       // load engine\")\n",
    "    print(\"  Cycle 2: load values        // load engine\")\n",
    "    print(\"  Cycle 3: compute addr       // alu engine\")\n",
    "    print(\"  Cycle 4: load tree[0]       // load engine\")\n",
    "    print(\"  Cycle 5: compute addr       // alu engine\")\n",
    "    print(\"  Cycle 6: load tree[1]       // load engine\")\n",
    "    print(\"  ... and so on\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Packed (multiple ops per cycle):\")\n",
    "    print(\"  Cycle 1: { load: [vload indices, vload values] }\")\n",
    "    print(\"  Cycle 2: { load: [load tree[0], load tree[1]], alu: [compute addrs...] }\")\n",
    "    print(\"  Cycle 3: { load: [load tree[2], load tree[3]], alu: [xor vals...] }\")\n",
    "    print(\"  ... operations overlap\")\n",
    "    print()\n",
    "    \n",
    "    print(\"VLIW instruction format example:\")\n",
    "    example_instr = {\n",
    "        \"valu\": [(\"^\", 40, 24, 32), (\"+\", 48, 40, 56)],  # 2 vector ops\n",
    "        \"load\": [(\"load\", 100, 101), (\"load\", 102, 103)], # 2 loads\n",
    "        \"alu\": [(\"+\", 10, 11, 12)]  # 1 scalar op\n",
    "    }\n",
    "    print(f\"  {example_instr}\")\n",
    "    print()\n",
    "    print(\"This single instruction does:\")\n",
    "    print(\"  - 2 vector XOR/ADD operations (16 elements total)\")\n",
    "    print(\"  - 2 memory loads\")\n",
    "    print(\"  - 1 scalar addition\")\n",
    "    print(\"  All in ONE cycle!\")\n",
    "\n",
    "show_vliw_packing_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hash Pipeline Opportunity\n",
    "\n",
    "The hash function has 6 stages that must be done sequentially (each stage depends on the previous). But we can still pack operations *within* each stage, and we can overlap hash computation with memory operations.\n",
    "\n",
    "Even better: we can be processing round N+1's loads while computing round N's hash!"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Optimization Strategy 3: Software Pipelining\n\n**Why do we need this?** VLIW packing got us to ~3,500 cycles. But there is still a fundamental problem: we process each vector group completely before starting the next one. The hash takes 12 cycles, but we only need 4 cycles of loads. That means the load slots sit idle for 8 cycles!\n\n**The key insight:** Memory is the bottleneck (we established this at the start). The hash computation is slow due to dependencies, but it uses vALU slots. The loads use load slots. These are DIFFERENT resources. We can use them simultaneously!\n\n**Target:** Get from ~3,500 cycles to ~1,500 cycles (about 2.3x improvement).\n\nThis is called **software pipelining** - manually scheduling operations so that while one vector group is computing its hash, the next group is loading its data."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def illustrate_software_pipeline():\n    \"\"\"Show how pipelining overlaps different stages with concrete cycles.\"\"\"\n    \n    print(\"SOFTWARE PIPELINING: The Key to 100x Speedup\")\n    print(\"=\" * 60)\n    print()\n    print(\"The idea: While computing hash for group N, load data for group N+1.\")\n    print(\"This hides memory latency behind computation.\")\n    print()\n    \n    print(\"CONCRETE CYCLE-BY-CYCLE SCHEDULE (simplified):\")\n    print(\"-\" * 60)\n    print()\n    print(\"Legend: G0 = Group 0, G1 = Group 1, etc.\")\n    print(\"        L = Load phase, H = Hash phase, S = Store phase\")\n    print()\n    print(\"Cycle  | Load Slots (2)        | vALU Slots (6)          | Notes\")\n    print(\"-------|----------------------|-------------------------|------------------\")\n    print(\"  1    | vload G0 idx,val     | --                      | Start G0\")\n    print(\"  2    | load G0 tree[0,1]    | --                      | Scattered loads\")\n    print(\"  3    | load G0 tree[2,3]    | --                      |\")\n    print(\"  4    | load G0 tree[4,5]    | XOR G0                  | Compute can start\")\n    print(\"  5    | load G0 tree[6,7]    | Hash G0 stg0 (ops 1,2)  |\")\n    print(\"  6    | vload G1 idx,val     | Hash G0 stg0 (op 3)     | G1 loads begin!\")\n    print(\"  7    | load G1 tree[0,1]    | Hash G0 stg1            |\")\n    print(\"  8    | load G1 tree[2,3]    | Hash G0 stg1            |\")\n    print(\"  9    | load G1 tree[4,5]    | Hash G0 stg2, XOR G1    | G1 compute starts\")\n    print(\" 10    | load G1 tree[6,7]    | Hash G0 stg2, Hash G1   |\")\n    print(\" 11    | vload G2 idx,val     | Hash G0 stg3, Hash G1   | G2 loads begin!\")\n    print(\" 12    | load G2 tree[0,1]    | Hash G0 stg3, Hash G1   |\")\n    print(\" ...   | (continues)          | (multiple in flight)    |\")\n    print()\n    \n    print(\"KEY INSIGHT: After the pipeline fills (~5 cycles), we process\")\n    print(\"one vector group every ~3 cycles in steady state!\")\n    print()\n    \n    # Calculate the math\n    BATCH_SIZE = 256\n    ROUNDS = 16\n    vector_groups = BATCH_SIZE // VLEN\n    total_iterations = vector_groups * ROUNDS\n    \n    # Without pipelining\n    cycles_per_group_unpipelined = 4 + 12 + 3  # load + hash + store/index\n    total_unpipelined = total_iterations * cycles_per_group_unpipelined\n    \n    # With pipelining - memory bound at ~3 cycles per group in steady state\n    # (4 scattered loads need 4 cycles, but we have 2 load slots, so need 4/2=2 cycles\n    # for tree loads, plus 1 for vloads = 3 cycles per group when pipelined)\n    cycles_per_group_pipelined = 3\n    pipeline_fill = 5\n    pipeline_drain = 5\n    total_pipelined = total_iterations * cycles_per_group_pipelined + pipeline_fill + pipeline_drain\n    \n    print(f\"Without pipelining: {total_iterations} groups x {cycles_per_group_unpipelined} = {total_unpipelined:,} cycles\")\n    print(f\"With pipelining:    {total_iterations} groups x {cycles_per_group_pipelined} + overhead = {total_pipelined:,} cycles\")\n    print()\n    print(f\"Pipelining speedup: {total_unpipelined / total_pipelined:.1f}x\")\n    print()\n    print(\"This gets us into the ~1,500 cycle range!\")\n\nillustrate_software_pipeline()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Instruction Format\n",
    "\n",
    "Before you start implementing, you need to understand how instructions are encoded. Let us examine the actual format used by the `KernelBuilder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instruction_format():\n",
    "    \"\"\"Explain how to write instructions for the machine.\"\"\"\n",
    "    \n",
    "    print(\"Instruction Bundle Format:\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"Each cycle executes ONE instruction bundle.\")\n",
    "    print(\"A bundle is a dict mapping engine names to lists of operations.\")\n",
    "    print()\n",
    "    print(\"Example bundle:\")\n",
    "    example = {\n",
    "        \"valu\": [(\"^\", 40, 24, 32), (\"+\", 48, 40, 56)],\n",
    "        \"load\": [(\"const\", 100, 0x7ED55D16), (\"load\", 102, 103)],\n",
    "        \"alu\": [(\"+\", 10, 11, 12)]\n",
    "    }\n",
    "    for engine, ops in example.items():\n",
    "        print(f\"  \\\"{engine}\\\": {ops}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Operation tuple format:\")\n",
    "    print(\"  (opcode, dest, src1, src2)   for ALU/vALU ops\")\n",
    "    print(\"  (\\\"load\\\", dest, addr)          for load\")\n",
    "    print(\"  (\\\"vload\\\", dest, addr)         for vector load (addr is scalar)\")\n",
    "    print(\"  (\\\"store\\\", addr, src)          for store\")\n",
    "    print(\"  (\\\"vstore\\\", addr, src)         for vector store\")\n",
    "    print(\"  (\\\"const\\\", dest, value)        for loading constants\")\n",
    "    print(\"  (\\\"select\\\", dest, cond, t, f)  for conditional select\")\n",
    "    print(\"  (\\\"vselect\\\", dest, cond, t, f) for vector conditional select\")\n",
    "    print()\n",
    "    \n",
    "    print(\"All numbers are scratch space addresses (0 to 1535).\")\n",
    "    print(\"For vector operations, address N refers to N, N+1, ..., N+7.\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Available ALU operations:\")\n",
    "    ops = [\"+\", \"-\", \"*\", \"//\", \"^\", \"&\", \"|\", \"<<\", \">>\", \"%\", \"<\", \"==\"]\n",
    "    print(f\"  {ops}\")\n",
    "\n",
    "explain_instruction_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch Space Management\n",
    "\n",
    "The scratch space is your \"register file\" - all operations read from and write to scratch addresses. You need to manage this space carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Minimal Working Example: From Scalar to Vectorized\n\nThis is the most important section for understanding the transformation. We show exactly how baseline scalar code becomes optimized vectorized VLIW code.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def show_code_transformation():\n    \"\"\"Show the actual instruction transformation from scalar to vectorized.\"\"\"\n    \n    print(\"TRANSFORMATION EXAMPLE: Loading and XORing batch data\")\n    print(\"=\" * 70)\n    print()\n    \n    print(\"BASELINE (scalar, 1 op per instruction):\")\n    print(\"-\" * 70)\n    baseline_instrs = [\n        '{\"load\": [(\"const\", tmp1, 0)]}           # tmp1 = 0 (batch index)',\n        '{\"alu\":  [(\"+\", addr, inp_indices_p, tmp1)]}  # addr = inp_indices_p + 0',\n        '{\"load\": [(\"load\", idx, addr)]}              # idx = mem[addr]',\n        '{\"alu\":  [(\"+\", addr, inp_values_p, tmp1)]}  # addr = inp_values_p + 0',\n        '{\"load\": [(\"load\", val, addr)]}              # val = mem[addr]',\n        '{\"alu\":  [(\"+\", tree_addr, forest_p, idx)]}  # tree_addr = forest_p + idx',\n        '{\"load\": [(\"load\", node_val, tree_addr)]}    # node_val = mem[tree_addr]',\n        '{\"alu\":  [(\"^\", val, val, node_val)]}        # val = val ^ node_val',\n        '# ... repeat for batch items 1, 2, 3, 4, 5, 6, 7 ...',\n    ]\n    for i, instr in enumerate(baseline_instrs, 1):\n        print(f\"  Cycle {i}: {instr}\")\n    print()\n    print(f\"  Total for 8 items: 8 x 8 = 64+ cycles (plus hash overhead)\")\n    print()\n    \n    print(\"OPTIMIZED (vectorized VLIW, multiple ops per instruction):\")\n    print(\"-\" * 70)\n    optimized_instrs = [\n        ('{\"alu\": [(\"+\", idx_base, inp_indices_p, batch_offset),\\n'\n         '         (\"+\", val_base, inp_values_p, batch_offset)]}'),\n        '{\"load\": [(\"vload\", v_idx, idx_base), (\"vload\", v_val, val_base)]}',\n        '{\"valu\": [(\"+\", v_tree_addr, v_forest_p, v_idx)]}',\n        '{\"alu\": [(\"+\", addr0, v_tree_addr+0, zero), (+, addr1, v_tree_addr+1, zero), ...]}',\n        '{\"load\": [(\"load\", v_node+0, addr0), (\"load\", v_node+1, addr1)]}',\n        '{\"load\": [(\"load\", v_node+2, addr2), (\"load\", v_node+3, addr3)]}',\n        '{\"load\": [(\"load\", v_node+4, addr4), (\"load\", v_node+5, addr5)]}',\n        '{\"load\": [(\"load\", v_node+6, addr6), (\"load\", v_node+7, addr7)]}',\n        '{\"valu\": [(\"^\", v_val, v_val, v_node)]}',\n    ]\n    for i, instr in enumerate(optimized_instrs, 1):\n        print(f\"  Cycle {i}: {instr}\")\n    print()\n    print(f\"  Total for 8 items: 9 cycles (not 64!)\")\n    print()\n    \n    print(\"KEY TRANSFORMATIONS APPLIED:\")\n    print(\"  1. vload/vstore: Load 8 contiguous values in 1 cycle (vs 8 cycles)\")\n    print(\"  2. valu: XOR 8 values in 1 cycle (vs 8 cycles)\")\n    print(\"  3. VLIW packing: Compute both addresses in cycle 1 (vs 2 cycles)\")\n    print(\"  4. Parallel loads: 2 scattered loads per cycle (vs 1)\")\n    print()\n    \n    print(\"WHAT THIS LOOKS LIKE IN THE KERNEL BUILDER:\")\n    print(\"-\" * 70)\n    print('''\n# Vector load batch data (2 vloads in 1 instruction)\ninstrs.append({\n    \"load\": [\n        (\"vload\", v_idx, idx_base),    # Load 8 indices\n        (\"vload\", v_val, val_base)     # Load 8 values\n    ]\n})\n\n# Compute tree addresses (vector add)\ninstrs.append({\n    \"valu\": [(\"+\", v_tree_addr, v_forest_p, v_idx)]\n})\n\n# Scattered tree loads (2 per cycle, takes 4 cycles for 8 loads)\nfor i in range(0, VLEN, 2):\n    instrs.append({\n        \"load\": [\n            (\"load\", v_node + i, addr_scalars[i]),\n            (\"load\", v_node + i + 1, addr_scalars[i + 1])\n        ]\n    })\n\n# Vector XOR (all 8 in 1 cycle)\ninstrs.append({\n    \"valu\": [(\"^\", v_val, v_val, v_node)]\n})\n''')\n\nshow_code_transformation()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## The Critical Bottleneck (Recap)\n\nWe front-loaded this insight at the beginning, but now that we understand all the optimizations, let us verify our bottleneck analysis with concrete numbers."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def identify_bottleneck():\n    \"\"\"Reinforce the memory bottleneck with concrete numbers.\"\"\"\n    \n    BATCH_SIZE = 256\n    ROUNDS = 16\n    vector_groups = BATCH_SIZE // VLEN\n    total_iterations = vector_groups * ROUNDS\n    \n    print(\"MEMORY BOTTLENECK ANALYSIS (RECAP)\")\n    print(\"=\" * 50)\n    print()\n    print(\"We established early that memory is the bottleneck.\")\n    print(\"Now let us quantify exactly how tight this constraint is.\")\n    print()\n    \n    # Irreducible memory operations\n    tree_loads = VLEN  # Must load 8 tree values\n    batch_loads = 2    # 1 vload for indices, 1 for values\n    batch_stores = 2   # 1 vstore each\n    \n    print(\"Per vector group (irreducible work):\")\n    print(f\"  Scattered tree loads:          {tree_loads}\")\n    print(f\"  Contiguous batch loads:        {batch_loads} (vload)\")\n    print(f\"  Contiguous batch stores:       {batch_stores} (vstore)\")\n    print()\n    \n    # Memory time calculation\n    load_slots = 2\n    store_slots = 2\n    \n    # Tree loads are scattered (cannot use vload), need individual loads\n    tree_load_cycles = (tree_loads + load_slots - 1) // load_slots  # = 4 cycles\n    batch_load_cycles = (batch_loads + load_slots - 1) // load_slots  # = 1 cycle\n    total_load_cycles = tree_load_cycles + batch_load_cycles\n    \n    print(f\"Load cycles needed per group:\")\n    print(f\"  Tree loads (scattered):   {tree_loads} / {load_slots} slots = {tree_load_cycles} cycles\")\n    print(f\"  Batch loads (contiguous): {batch_loads} / {load_slots} slots = {batch_load_cycles} cycle\")\n    print(f\"  Total load cycles:        {total_load_cycles} cycles\")\n    print()\n    \n    # Theoretical minimum\n    theoretical_min = total_iterations * tree_load_cycles\n    print(f\"THEORETICAL MINIMUM (memory-bound):\")\n    print(f\"  {total_iterations} groups x {tree_load_cycles} cycles = {theoretical_min:,} cycles\")\n    print()\n    \n    # With stores overlapped\n    print(\"But stores can overlap with next iteration's loads!\")\n    print(\"With perfect pipelining, we approach the load-limited minimum.\")\n    print()\n    \n    # Hash comparison\n    print(\"COMPUTE vs MEMORY comparison:\")\n    print(f\"  Hash computation: 12 cycles (dependency limited)\")\n    print(f\"  Tree loads:       {tree_load_cycles} cycles (bandwidth limited)\")\n    print(f\"  Ratio: 12 / {tree_load_cycles} = {12/tree_load_cycles:.1f}x\")\n    print()\n    print(\"We have 3x more compute time than load time!\")\n    print(\"This is WHY pipelining works - we hide loads behind compute.\")\n    print()\n    print(\"TARGET: ~1,500 cycles (includes pipeline fill/drain overhead)\")\n\nidentify_bottleneck()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_optimized_cycles():\n",
    "    \"\"\"Estimate achievable cycles with full optimization.\"\"\"\n",
    "    \n",
    "    BATCH_SIZE = 256\n",
    "    ROUNDS = 16\n",
    "    BASELINE = 147734\n",
    "    TARGET = 1500\n",
    "    \n",
    "    vector_groups = BATCH_SIZE // VLEN  # 32\n",
    "    total_iterations = vector_groups * ROUNDS  # 512\n",
    "    \n",
    "    print(\"Key bottleneck analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    \n",
    "    # Memory bottleneck\n",
    "    print(\"Memory operations per iteration:\")\n",
    "    loads_per_iter = 8 + 2  # 8 scattered tree loads, 2 vloads for batch data\n",
    "    stores_per_iter = 8 + 2  # 8 scattered stores (if needed), 2 vstores\n",
    "    print(f\"  Loads: ~{loads_per_iter} (8 tree + 2 batch)\")\n",
    "    print(f\"  Stores: ~{stores_per_iter} (could optimize)\")\n",
    "    print(f\"  With 2 load slots: {loads_per_iter // 2} cycles for loads\")\n",
    "    print()\n",
    "    \n",
    "    # Compute bottleneck\n",
    "    print(\"Compute operations per iteration:\")\n",
    "    print(f\"  Hash: 6 stages x 2 cycles = 12 cycles (dependency limited)\")\n",
    "    print(f\"  Other: ~4 cycles (XOR, index calc, etc.)\")\n",
    "    print()\n",
    "    \n",
    "    # With pipelining\n",
    "    print(\"With software pipelining:\")\n",
    "    print(\"  Can overlap loads with computation\")\n",
    "    print(\"  Effective cycles per iteration: ~3-5 (in steady state)\")\n",
    "    print()\n",
    "    \n",
    "    # Rough estimates\n",
    "    cycles_per_iter_basic = 16  # Conservative\n",
    "    cycles_per_iter_pipelined = 4  # Aggressive\n",
    "    \n",
    "    basic_total = total_iterations * cycles_per_iter_basic\n",
    "    pipelined_total = total_iterations * cycles_per_iter_pipelined + 50  # Overhead\n",
    "    \n",
    "    print(\"Estimated total cycles:\")\n",
    "    print(f\"  Conservative (no deep pipelining): ~{basic_total:,}\")\n",
    "    print(f\"  Aggressive (full pipelining):      ~{pipelined_total:,}\")\n",
    "    print()\n",
    "    print(f\"  Target: {TARGET:,}\")\n",
    "    print(f\"  Baseline: {BASELINE:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"To reach ~1500 cycles:\")\n",
    "    cycles_per_iter_needed = (TARGET - 50) / total_iterations\n",
    "    print(f\"  Need ~{cycles_per_iter_needed:.1f} cycles per iteration\")\n",
    "    print(\"  This requires aggressive pipelining and optimal scheduling.\")\n",
    "\n",
    "estimate_optimized_cycles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running and Testing Your Implementation\n",
    "\n",
    "Here is how to test your optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_testing_workflow():\n",
    "    \"\"\"Explain how to test implementations.\"\"\"\n",
    "    \n",
    "    print(\"Testing workflow:\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"1. Quick cycle count check:\")\n",
    "    print(\"   python perf_takehome.py Tests.test_kernel_cycles\")\n",
    "    print()\n",
    "    print(\"2. Full validation (required for submission):\")\n",
    "    print(\"   python tests/submission_tests.py\")\n",
    "    print()\n",
    "    print(\"3. Visual debugging with Perfetto trace:\")\n",
    "    print(\"   python perf_takehome.py Tests.test_kernel_trace\")\n",
    "    print(\"   python watch_trace.py  # Opens browser\")\n",
    "    print()\n",
    "    print(\"4. Verify tests folder unchanged:\")\n",
    "    print(\"   git diff origin/main tests/\")\n",
    "    print()\n",
    "    print(\"Performance thresholds (from submission_tests.py):\")\n",
    "    print(\"  - Pass all tests for correctness\")\n",
    "    print(\"  - Target: < 1500 cycles for impressive result\")\n",
    "    print(\"  - Best human: 1790 cycles (in 2 hours)\")\n",
    "    print(\"  - Best AI: 1363 cycles (Claude Opus 4.5)\")\n",
    "\n",
    "show_testing_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: Understanding the Baseline Code\n",
    "\n",
    "Let us look at how the baseline implementation works. Understanding this is essential before you can improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_understand_baseline_structure():\n",
    "    \"\"\"Verify understanding of baseline code structure.\"\"\"\n",
    "    \n",
    "    # The baseline is in perf_takehome.py, in KernelBuilder.build_kernel()\n",
    "    # It generates instructions for a scalar implementation\n",
    "    \n",
    "    # Question: How many instruction bundles does the baseline generate\n",
    "    # for the full test case (256 batch, 16 rounds)?\n",
    "    \n",
    "    # Let's count:\n",
    "    # - Setup instructions: ~20 (loading constants, addresses)\n",
    "    # - Per (batch_item, round) pair:\n",
    "    #   - Load idx, load val: ~4 instructions\n",
    "    #   - Load node_val: ~2 instructions  \n",
    "    #   - XOR: 1 instruction\n",
    "    #   - Hash (6 stages x 3 ops): ~18 instructions\n",
    "    #   - Index calculation: ~5 instructions\n",
    "    #   - Boundary check: ~2 instructions\n",
    "    #   - Stores: ~4 instructions\n",
    "    #   Total: ~36 instructions per inner iteration\n",
    "    \n",
    "    BATCH_SIZE = 256\n",
    "    ROUNDS = 16\n",
    "    \n",
    "    setup_instrs = 20\n",
    "    per_iteration = 36\n",
    "    total_iterations = BATCH_SIZE * ROUNDS\n",
    "    \n",
    "    estimated_total = setup_instrs + per_iteration * total_iterations\n",
    "    \n",
    "    print(\"Baseline instruction count estimate:\")\n",
    "    print(f\"  Setup: ~{setup_instrs}\")\n",
    "    print(f\"  Per iteration: ~{per_iteration}\")\n",
    "    print(f\"  Iterations: {total_iterations}\")\n",
    "    print(f\"  Total: ~{estimated_total:,}\")\n",
    "    print()\n",
    "    print(f\"  Actual baseline cycles: 147,734\")\n",
    "    print(f\"  Our estimate: {estimated_total:,}\")\n",
    "    print(f\"  (Close enough - differences from address calculations and debug ops)\")\n",
    "    \n",
    "    # Assert our understanding is in the right ballpark\n",
    "    assert 100000 < estimated_total < 200000, \"Estimate way off - review the code\"\n",
    "    print()\n",
    "    print(\"TEST PASSED: Understanding of baseline structure is correct.\")\n",
    "\n",
    "test_understand_baseline_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: Identifying Parallelism\n",
    "\n",
    "Can you identify which operations in the inner loop can be parallelized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_identify_parallelism():\n",
    "    \"\"\"Test understanding of parallelism opportunities.\"\"\"\n",
    "    \n",
    "    # Consider processing batch items 0-7 simultaneously\n",
    "    # Which of these operations can be done in parallel across all 8?\n",
    "    \n",
    "    operations = {\n",
    "        \"load_indices\": True,      # Yes - independent memory locations\n",
    "        \"load_values\": True,       # Yes - independent memory locations\n",
    "        \"load_tree_values\": True,  # Yes - though scattered, still independent\n",
    "        \"xor_with_node\": True,     # Yes - pure SIMD\n",
    "        \"hash_computation\": True,  # Yes - same ops on independent data\n",
    "        \"index_calculation\": True, # Yes - same ops on independent data\n",
    "        \"store_results\": True,     # Yes - independent memory locations\n",
    "    }\n",
    "    \n",
    "    print(\"Parallelism analysis (batch items 0-7):\")\n",
    "    print()\n",
    "    \n",
    "    for op, parallel in operations.items():\n",
    "        status = \"[PARALLEL]\" if parallel else \"[SEQUENTIAL]\"\n",
    "        print(f\"  {op:25s} {status}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"KEY INSIGHT: ALL inner loop operations parallelize across batch items!\")\n",
    "    print(\"This is why SIMD (vectorization) is so effective here.\")\n",
    "    print()\n",
    "    print(\"The only sequential dependency is within each batch item:\")\n",
    "    print(\"  round N must complete before round N+1 can start.\")\n",
    "    print()\n",
    "    \n",
    "    # Verify understanding\n",
    "    assert all(operations.values()), \"Some operations incorrectly marked as sequential\"\n",
    "    print(\"TEST PASSED: Parallelism opportunities correctly identified.\")\n",
    "\n",
    "test_identify_parallelism()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def summarize_optimization_path():\n    \"\"\"Summarize all optimizations and their cumulative effect with cycle targets.\"\"\"\n    \n    print(\"THE PATH TO 100X SPEEDUP - CONCRETE MILESTONES\")\n    print(\"=\" * 60)\n    print()\n    \n    BASELINE = 147734\n    \n    milestones = [\n        (\"Scalar baseline\", 147734, \"1 instruction per cycle\"),\n        (\"Vectorized (8x SIMD)\", 7700, \"vload/vstore/valu process 8 items\"),\n        (\"+ VLIW packing\", 3500, \"Multiple ops per instruction\"),\n        (\"+ Software pipelining\", 1500, \"Overlap loads with hash computation\"),\n    ]\n    \n    print(\"CYCLE MILESTONES (your targets):\")\n    print(\"-\" * 60)\n    \n    prev_cycles = None\n    for name, cycles, technique in milestones:\n        if prev_cycles:\n            improvement = prev_cycles / cycles\n            print(f\"  {name:25s} ~{cycles:,} cycles  ({improvement:.1f}x from previous)\")\n        else:\n            print(f\"  {name:25s} {cycles:,} cycles  (starting point)\")\n        print(f\"    Technique: {technique}\")\n        print()\n        prev_cycles = cycles\n    \n    total_speedup = BASELINE / 1500\n    print(f\"Total speedup: {total_speedup:.0f}x\")\n    print()\n    \n    print(\"WHICH OPTIMIZATION GIVES WHAT:\")\n    print(\"-\" * 60)\n    print(\"  Vectorization (8x):     147,734 / 8  = ~18,000 cycles theoretical\")\n    print(\"                          Actual ~7,700 (includes overhead)\")\n    print()\n    print(\"  VLIW packing (~2x):     7,700 / 2   = ~3,500 cycles\")\n    print(\"                          Pack loads, ALU ops into fewer instructions\")\n    print()\n    print(\"  Pipelining (~2.3x):     3,500 / 2.3 = ~1,500 cycles\")\n    print(\"                          Hide 12-cycle hash behind 4-cycle loads\")\n    print()\n    \n    print(\"THE MEMORY BOTTLENECK DEFINES THE LIMIT:\")\n    print(\"-\" * 60)\n    print(\"  Per vector group: 8 scattered tree loads\")\n    print(\"  With 2 load slots: 4 cycles minimum for loads\")\n    print(\"  512 vector groups total: 512 x 4 = 2,048 cycles minimum\")\n    print()\n    print(\"  But with perfect pipelining and dual-batch processing:\")\n    print(\"  You can approach ~1,500 cycles or even lower!\")\n    print()\n    \n    print(\"Best known results:\")\n    print(\"  - Human (2 hrs): 1,790 cycles\")\n    print(\"  - Claude Opus 4.5: 1,363 cycles\")\n    print(\"  - Target: <1,500 cycles\")\n\nsummarize_optimization_path()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_memory_layout():\n",
    "    \"\"\"Test understanding of memory layout.\"\"\"\n",
    "    \n",
    "    # The memory image has a header, then tree values, then batch data\n",
    "    # Header format (7 words at addresses 0-6):\n",
    "    #   [0]: rounds\n",
    "    #   [1]: n_nodes\n",
    "    #   [2]: batch_size\n",
    "    #   [3]: forest_height\n",
    "    #   [4]: forest_values_p (pointer to tree data)\n",
    "    #   [5]: inp_indices_p (pointer to batch indices)\n",
    "    #   [6]: inp_values_p (pointer to batch values)\n",
    "    \n",
    "    # For our test case:\n",
    "    tree_height = 10\n",
    "    n_nodes = 2**(tree_height + 1) - 1  # 2047\n",
    "    batch_size = 256\n",
    "    \n",
    "    header_size = 7\n",
    "    forest_values_p = header_size  # = 7\n",
    "    inp_indices_p = forest_values_p + n_nodes  # = 7 + 2047 = 2054\n",
    "    inp_values_p = inp_indices_p + batch_size  # = 2054 + 256 = 2310\n",
    "    \n",
    "    print(\"Memory layout for test case:\")\n",
    "    print(f\"  Tree height: {tree_height}\")\n",
    "    print(f\"  Tree nodes: {n_nodes}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print()\n",
    "    print(\"Address ranges:\")\n",
    "    print(f\"  Header:        [0, 6]\")\n",
    "    print(f\"  Tree values:   [{forest_values_p}, {forest_values_p + n_nodes - 1}]\")\n",
    "    print(f\"  Batch indices: [{inp_indices_p}, {inp_indices_p + batch_size - 1}]\")\n",
    "    print(f\"  Batch values:  [{inp_values_p}, {inp_values_p + batch_size - 1}]\")\n",
    "    print()\n",
    "    \n",
    "    # To load batch items 0-7:\n",
    "    print(\"To load batch items 0-7:\")\n",
    "    print(f\"  vload indices from address {inp_indices_p}\")\n",
    "    print(f\"  vload values from address {inp_values_p}\")\n",
    "    print()\n",
    "    \n",
    "    # To load tree value for batch item i:\n",
    "    print(\"To load tree value for batch item i:\")\n",
    "    print(f\"  address = {forest_values_p} + indices[i]\")\n",
    "    print(f\"  (This is why tree loads are scattered!)\")\n",
    "    \n",
    "    # Verify\n",
    "    assert n_nodes == 2047\n",
    "    assert inp_indices_p == 2054\n",
    "    assert inp_values_p == 2310\n",
    "    print()\n",
    "    print(\"TEST PASSED: Memory layout correctly understood.\")\n",
    "\n",
    "test_memory_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Path to 100x Speedup\n",
    "\n",
    "Let us recap the optimization strategies and their expected contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_optimization_path():\n",
    "    \"\"\"Summarize all optimizations and their cumulative effect.\"\"\"\n",
    "    \n",
    "    print(\"THE PATH TO 100X SPEEDUP\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    \n",
    "    BASELINE = 147734\n",
    "    current = BASELINE\n",
    "    \n",
    "    optimizations = [\n",
    "        (\"Vectorization (SIMD)\", 8, \n",
    "         \"Process 8 batch items per vector operation\"),\n",
    "        (\"VLIW packing\", 3,\n",
    "         \"Use multiple slots per cycle (12 ALU, 6 vALU, 2 load, 2 store)\"),\n",
    "        (\"Loop unrolling\", 1.5,\n",
    "         \"Eliminate per-iteration overhead, enable better scheduling\"),\n",
    "        (\"Software pipelining\", 2,\n",
    "         \"Overlap loads with computation, hide memory latency\"),\n",
    "    ]\n",
    "    \n",
    "    print(f\"Starting point: {BASELINE:,} cycles\")\n",
    "    print()\n",
    "    \n",
    "    for name, speedup, description in optimizations:\n",
    "        current = current / speedup\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  {description}\")\n",
    "        print(f\"  Speedup: {speedup}x -> {int(current):,} cycles\")\n",
    "        print()\n",
    "    \n",
    "    total_speedup = BASELINE / current\n",
    "    print(f\"Cumulative speedup: {total_speedup:.0f}x\")\n",
    "    print(f\"Estimated final: ~{int(current):,} cycles\")\n",
    "    print()\n",
    "    print(\"Note: These multipliers are rough estimates.\")\n",
    "    print(\"Actual results depend on implementation quality.\")\n",
    "    print()\n",
    "    print(\"Best known results:\")\n",
    "    print(\"  - Human (2 hrs): 1,790 cycles\")\n",
    "    print(\"  - Claude Opus 4.5: 1,363 cycles\")\n",
    "    print(\"  - Target: <1,500 cycles\")\n",
    "\n",
    "summarize_optimization_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You now understand:\n",
    "\n",
    "1. **What the kernel computes**: Tree traversal with hash-based direction selection\n",
    "2. **Why the baseline is slow**: Uses 1 slot per cycle out of dozens available\n",
    "3. **How to vectorize**: Process 8 batch items at once with vALU operations\n",
    "4. **How to pack instructions**: Use VLIW to issue multiple operations per cycle\n",
    "5. **How to pipeline**: Overlap memory operations with computation\n",
    "\n",
    "The implementation challenge is translating this understanding into actual code in `perf_takehome.py`. Modify the `KernelBuilder.build_kernel()` method to generate optimized instructions.\n",
    "\n",
    "Start with vectorization (the biggest win), then add VLIW packing, then work on deeper pipelining.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary of key numbers to remember\n",
    "print(\"KEY NUMBERS TO REMEMBER:\")\n",
    "print(f\"  VLEN = {VLEN}\")\n",
    "print(f\"  Batch size = 256 = 32 x {VLEN}\")\n",
    "print(f\"  Rounds = 16\")\n",
    "print(f\"  Tree nodes = 2047\")\n",
    "print(f\"  Scratch = {SCRATCH_SIZE} words\")\n",
    "print()\n",
    "print(\"  Baseline: 147,734 cycles\")\n",
    "print(\"  Target: ~1,500 cycles\")\n",
    "print(\"  Required speedup: ~100x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_lexer": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}